# -*- coding: utf-8 -*-
"""CP 05 - FrontEnd - CLuster + Streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GgA20UdRSutcNBHAlHqnVwSclYDYfVVZ

# CP

## Imports
"""

#!pip install pycaret

"""## Modelo 1 - REAL"""

nltk.download('stopwords')
from transformers                     import BertTokenizer, BertForSequenceClassification, AdamW, BertModel
from yellowbrick.cluster              import KElbowVisualizer, SilhouetteVisualizer
from sklearn.preprocessing            import LabelEncoder, MinMaxScaler
from sklearn.feature_extraction.text  import TfidfVectorizer
from sklearn.metrics                  import silhouette_score
from sklearn.cluster                  import KMeans
from pycaret.clustering               import *
from nltk.corpus                      import stopwords

import matplotlib.pyplot  as plt
import seaborn            as sns
import pandas             as pd
import numpy              as np
import torch
import joblib
import nltk

"""### Analise exploratoria"""

df = pd.read_csv('all_movies.csv', sep=';')
print("Dimensões do dataset:", df.shape)
print("Colunas do dataset:", df.columns)

# Estatísticas descritivas
print(df.describe())

# Visualização da distribuição de gêneros
plt.figure(figsize=(10,6))
sns.countplot(y=df['genre'], order=df['genre'].value_counts().index)
plt.title("Distribuição de Gêneros")
plt.show()

"""### Tratamento dos dados"""

# Removendo valores nulos
df.dropna(inplace=True)

# Removendo Stopwords das sinopses
df['sinopse_clean'] = df['sinopse'].apply(lambda x: ' '.join([word for word in x.split() if word not in set(stopwords.words('portuguese'))]))

# Encoding para os títulos
encoder = LabelEncoder()
df['title_pt_encoded'] = encoder.fit_transform(df['title_pt'])

# Tokenização com BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

"""### Modelo 3 - SKLearn + BERT"""

def fine_tune_bert(texts, labels):
    model.train()
    optimizer = AdamW(model.parameters(), lr=5e-5)

    # Exemplo de treinamento sem o argumento 'labels'
    for text in texts:
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
        outputs = model(**inputs)  # Sem labels
        loss = outputs.last_hidden_state.mean()  # Usando uma métrica de exemplo para cálculo de perda
        loss.backward()
        optimizer.step()
    return model

# Fine-tuning com algumas sinopses como exemplo (processo pode ser mais longo)
texts = df['sinopse_clean'].values[:100]
labels = np.random.randint(0, 3, size=(100,))  # Apenas para fins de exemplo
fine_tune_bert(texts, labels)

# Vetorizando as sinopses com BERT após fine-tuning
def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()  # Tirando a média da última camada oculta para cada frase

embeddings = np.vstack([embed_text(text) for text in df['sinopse_clean']])

"""### Modelo"""

# Modelo de cluster usando KMeans do SKLearn
kmeans = KMeans()

# KElbowVisualizer para encontrar o número ideal de clusters
visualizer = KElbowVisualizer(kmeans, k=(2,20))
visualizer.fit(embeddings)
visualizer.show()

# Ajustando o KMeans com o número ideal de clusters
kmeans = KMeans(n_clusters=visualizer.elbow_value_, init='k-means++', max_iter=500, n_init=50)
clusters = kmeans.fit_predict(embeddings)

joblib.dump(kmeans, 'kmeans_model_2.0.joblib')

"""### Avaliação"""

# Atribuindo os clusters ao DataFrame
df['Cluster'] = clusters

# Salvando o DataFrame atualizado com a coluna 'Cluster'
df.to_csv('all_movies_with_clusters.csv', sep=';', index=False)

# Exibindo os primeiros 5 resultados com seus clusters
df.head()

# Visualização dos clusters - Contagem de filmes por cluster
plt.figure(figsize=(10,6))
sns.countplot(x='Cluster', data=df)
plt.title("Distribuição de Filmes por Cluster")
plt.show()

# Agrupando os dados por Cluster e Gênero
cluster_genre_counts = df.groupby(['Cluster', 'genre']).size().reset_index(name='count')

# Criando o gráfico de barras
plt.figure(figsize=(12,8))
sns.barplot(x='Cluster', y='count', hue='genre', data=cluster_genre_counts)
plt.title("Quantidade de Filmes por Gênero em Cada Cluster")
plt.xlabel("Cluster")
plt.ylabel("Quantidade de Filmes")
plt.legend(title="Gênero", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

"""## Modelo 2 - TESTE"""

import nltk
from transformers import BertTokenizer, BertModel
from yellowbrick.cluster import KElbowVisualizer
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import torch
import joblib
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA

# Baixando stopwords apenas se necessário
nltk.download('stopwords', quiet=True)

"""## Analise exploratoria"""

# Função para carregar dados e realizar análise exploratória
def load_and_explore_data(filepath):
    df = pd.read_csv(filepath, sep=';')
    print(f"Dimensões do dataset: {df.shape}")
    print(f"Colunas do dataset: {df.columns}")
    print(df.describe())

    # Visualização da distribuição de gêneros
    plt.figure(figsize=(10, 6))
    sns.countplot(y=df['genre'], order=df['genre'].value_counts().index)
    plt.title("Distribuição de Gêneros")
    plt.show()

    return df

"""## Tratamento dos dados"""

# Função para tratar dados
def clean_data(df):
    df.dropna(inplace=True)  # Remove missing values
    # Removendo stopwords das sinopses
    stop_words = set(stopwords.words('portuguese'))
    df['sinopse_clean'] = df['sinopse'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))

    # Encoding para os títulos
    encoder = LabelEncoder()
    df['title_pt_encoded'] = encoder.fit_transform(df['title_pt'])
    return df

"""### Uso do BERT"""

# Função para tokenizar e embedar com BERT
def embed_text(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()

# Função para calcular embeddings de todas as sinopses e adicionr no DataFrame
def calculate_embeddings(df, tokenizer, model):
    embeddings = np.vstack([embed_text(text, tokenizer, model) for text in df['sinopse_clean']])
    df['embeddings'] = list(embeddings)  # Salvando os embeddings no DataFrame
    return df

"""### PCA E Metodo do cotovelo"""

# Função para ajustar KMeans com PCA e Elbow Method
def cluster_data_with_kmeans(embeddings, n_clusters=None):
    # Redução de dimensionalidade com PCA
    pca = PCA(n_components=100)
    embeddings_pca = pca.fit_transform(embeddings)
    joblib.dump(pca, 'pca_model.joblib')

    # Elbow method para encontrar o número ideal de clusters
    kmeans = KMeans()
    visualizer = KElbowVisualizer(kmeans, k=(2, 30))
    visualizer.fit(embeddings_pca)
    visualizer.show()
    n_clusters = n_clusters or visualizer.elbow_value_
    print(f"Elbow method suggests {n_clusters} clusters.")

    # Ajustando o KMeans com o número de clusters
    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=500, n_init=50)
    clusters = kmeans.fit_predict(embeddings_pca)
    joblib.dump(kmeans, 'kmeans_model_2.0.joblib')
    return clusters

# Função para recomendar o filme mais semelhante com base em uma nova sinopse
def recommend_movie(new_synopsis, df, tokenizer, model, pca, kmeans):
    # Gerando embedding para a nova sinopse
    new_embedding = embed_text(new_synopsis, tokenizer, model)
    new_embedding_pca = pca.transform(new_embedding)  # Aplicando PCA na nova sinopse

    # Aplicando PCA nos embeddings dos filmes
    movie_embeddings_pca = np.vstack(df['embeddings'].values)
    movie_embeddings_pca = pca.transform(movie_embeddings_pca)

    # Identificando o cluster mais próximo para a nova sinopse
    cluster = kmeans.predict(new_embedding_pca)

    # Filtrando filmes no mesmo cluster
    same_cluster_movies = df[df['Cluster'] == cluster[0]]

    # Calculando similaridade por cosseno
    cosine_similarities = cosine_similarity(new_embedding_pca, movie_embeddings_pca[same_cluster_movies.index])
    most_similar_idx = np.argmax(cosine_similarities)

    # Retornando o filme mais similar
    recommended_movie = same_cluster_movies.iloc[most_similar_idx]['title_pt']
    return recommended_movie

"""### Salvando o modelo"""

# Função para salvar e visualizar clusters
def visualize_and_save_clusters(df, clusters):
    df['Cluster'] = clusters
    df.to_csv('all_movies_with_clusters.csv', sep=';', index=False)

    # Visualização da distribuição de filmes por cluster
    plt.figure(figsize=(10, 6))
    sns.countplot(x='Cluster', data=df)
    plt.title("Distribuição de Filmes por Cluster")
    plt.show()

    # Agrupando os dados por Cluster e Gênero
    cluster_genre_counts = df.groupby(['Cluster', 'genre']).size().reset_index(name='count')

    # Gráfico de distribuição de gêneros em cada cluster
    plt.figure(figsize=(12, 8))
    sns.barplot(x='Cluster', y='count', hue='genre', data=cluster_genre_counts)
    plt.title("Quantidade de Filmes por Gênero em Cada Cluster")
    plt.xlabel("Cluster")
    plt.ylabel("Quantidade de Filmes")
    plt.legend(title="Gênero", bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.show()

# Carregar e processar dados
df = load_and_explore_data('all_movies.csv')
df = clean_data(df)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
df = calculate_embeddings(df, tokenizer, model)
embeddings = np.vstack(df['embeddings'].values)
clusters = cluster_data_with_kmeans(embeddings)

# Visualizando e salvando resultados
visualize_and_save_clusters(df, clusters)

# Exemplo de recomendação de filme
# Carregar modelos já treinados
pca = joblib.load('pca_model.joblib')
kmeans = joblib.load('kmeans_model_2.0.joblib')

"""### Teste"""

# Exemplo de nova sinopse
new_synopsis = input("Insira a sinopse: ")

# Recomendar filme mais similar
recommended_movie = recommend_movie(new_synopsis, df, tokenizer, model, pca, kmeans)
print(f"Filme recomendado: {recommended_movie}")

